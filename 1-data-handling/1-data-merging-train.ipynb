{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df):\n",
    "    \"\"\"Reduce memory usage of a DataFrame by downcasting numerical columns.\"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Convert integers\n",
    "        if col_type in [\"int16\", \"int32\", \"int64\"]:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "        # Convert floats\n",
    "        elif col_type in [\"float16\", \"float32\", \"float64\"]:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "        # Convert object types to category if unique values are low\n",
    "        elif col_type == \"object\":\n",
    "            num_unique_values = df[col].nunique()\n",
    "            num_total_values = len(df[col])\n",
    "            if num_unique_values / num_total_values < 0.5:  # Threshold to convert to category\n",
    "                df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {(1 - end_mem / start_mem) * 100:.2f}%\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 127.68 MB\n",
      "Memory usage after optimization: 43.73 MB\n",
      "Reduced by 65.75%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m merchants \u001b[38;5;241m=\u001b[39m reduce_memory_usage(merchants)\n\u001b[1;32m      4\u001b[0m historical_transactions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../raw-data/historical_transactions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m historical_transactions \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_memory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistorical_transactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m new_merchant_transactions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../raw-data/new_merchant_transactions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m new_merchant_transactions \u001b[38;5;241m=\u001b[39m reduce_memory_usage(new_merchant_transactions)\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mreduce_memory_usage\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreduce_memory_usage\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce memory usage of a DataFrame by downcasting numerical columns.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     start_mem \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage before optimization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_mem\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/root/University/Y4S2/SC4000/project/elo-merchant/env/lib/python3.12/site-packages/pandas/core/frame.py:3756\u001b[0m, in \u001b[0;36mDataFrame.memory_usage\u001b[0;34m(self, index, deep)\u001b[0m\n\u001b[1;32m   3666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmemory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, deep: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   3667\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3668\u001b[0m \u001b[38;5;124;03m    Return the memory usage of each column in bytes.\u001b[39;00m\n\u001b[1;32m   3669\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3753\u001b[0m \u001b[38;5;124;03m    5244\u001b[39;00m\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3755\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(\n\u001b[0;32m-> 3756\u001b[0m         [\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()],\n\u001b[1;32m   3757\u001b[0m         index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m   3758\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp,\n\u001b[1;32m   3759\u001b[0m     )\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[1;32m   3761\u001b[0m         index_memory_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(\n\u001b[1;32m   3762\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmemory_usage(deep\u001b[38;5;241m=\u001b[39mdeep), index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3763\u001b[0m         )\n",
      "File \u001b[0;32m~/root/University/Y4S2/SC4000/project/elo-merchant/env/lib/python3.12/site-packages/pandas/core/series.py:5481\u001b[0m, in \u001b[0;36mSeries.memory_usage\u001b[0;34m(self, index, deep)\u001b[0m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmemory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, deep: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5434\u001b[0m \u001b[38;5;124;03m    Return the memory usage of the Series.\u001b[39;00m\n\u001b[1;32m   5435\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5479\u001b[0m \u001b[38;5;124;03m    244\u001b[39;00m\n\u001b[1;32m   5480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5481\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_memory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[1;32m   5483\u001b[0m         v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmemory_usage(deep\u001b[38;5;241m=\u001b[39mdeep)\n",
      "File \u001b[0;32m~/root/University/Y4S2/SC4000/project/elo-merchant/env/lib/python3.12/site-packages/pandas/core/base.py:1174\u001b[0m, in \u001b[0;36mIndexOpsMixin._memory_usage\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PYPY:\n\u001b[1;32m   1173\u001b[0m     values \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\n\u001b[0;32m-> 1174\u001b[0m     v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_usage_of_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merchants = pd.read_csv(\"../../raw-data/merchants.csv\")\n",
    "merchants = reduce_memory_usage(merchants)\n",
    "\n",
    "historical_transactions = pd.read_csv(\"../../raw-data/historical_transactions.csv\")\n",
    "historical_transactions = reduce_memory_usage(historical_transactions)\n",
    "\n",
    "new_merchant_transactions = pd.read_csv(\"../../raw-data/new_merchant_transactions.csv\")\n",
    "new_merchant_transactions = reduce_memory_usage(new_merchant_transactions)\n",
    "\n",
    "train = pd.read_csv(\"../../raw-data/train.csv\")\n",
    "train = reduce_memory_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names and data types for each dataframe\n",
    "for df_name, df in zip([\"merchants\", \"historical_transactions\", \"new_merchant_transactions\", \"train\"], \n",
    "                        [merchants, historical_transactions, new_merchant_transactions, train]):\n",
    "    print(f\"DataFrame: {df_name}\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of both dataframes\n",
    "print(f\"Size of historical_transactions: {historical_transactions.shape}\")\n",
    "print(f\"Size of new_merchant_transactions: {new_merchant_transactions.shape}\")\n",
    "\n",
    "# Combine both datasets\n",
    "transactions = pd.concat([historical_transactions, new_merchant_transactions], ignore_index=True)\n",
    "\n",
    "# Print the size of the combined dataframe\n",
    "print(f\"Size of combined transactions DataFrame: {transactions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove duplicate merchant_id from merchants, keeping only the first occurrence\n",
    "merchants_unique = merchants.drop_duplicates(subset=\"merchant_id\", keep=\"first\")\n",
    "\n",
    "# Step 2: Merge transactions with merchants on merchant_id\n",
    "transactions_merged = transactions.merge(merchants_unique, on=\"merchant_id\", how=\"left\")\n",
    "\n",
    "# Step 3: Print the shape of the merged DataFrame\n",
    "print(f\"Size of transactions_merged: {transactions_merged.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "display(transactions_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(transactions_merged.head(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge transactions_merged with train on card_id, keeping all columns from train\n",
    "final_df = transactions_merged.merge(train, on=\"card_id\", how=\"left\")\n",
    "\n",
    "# Step 2: Drop rows where target is NaN (card_id that don’t exist in train)\n",
    "final_df = final_df.dropna(subset=['target'])\n",
    "\n",
    "# Step 3: Reorder columns: card_id first, target last\n",
    "cols = ['card_id'] + [col for col in final_df.columns if col not in ['card_id', 'target']] + ['target']\n",
    "final_df = final_df[cols]\n",
    "\n",
    "# Step 4: Print the size of the final DataFrame\n",
    "print(f\"Size of final_df after dropping unmatched card_ids: {final_df.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"card_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = reduce_memory_usage(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all column names in final_df\n",
    "print(final_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Convert 'first_active_month' to number of months from today\n",
    "if 'first_active_month' in final_df.columns:\n",
    "    final_df['first_active_month'] = pd.to_datetime(final_df['first_active_month'])\n",
    "    today = datetime.today()\n",
    "    final_df['first_active_month'] = final_df['first_active_month'].apply(lambda x: (today.year - x.year) * 12 + (today.month - x.month))\n",
    "\n",
    "# Step 2: Drop 'first_active_month' and 'card_id'\n",
    "final_df = final_df.drop(columns=['first_active_month'], errors='ignore')\n",
    "\n",
    "# Step 3: Encode all categorical values into integers, excluding 'card_id'\n",
    "categorical_cols = final_df.select_dtypes(include=['category', 'object']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col != 'card_id']  # Exclude card_id\n",
    "\n",
    "for col in categorical_cols:\n",
    "    final_df[col] = final_df[col].astype('category').cat.codes\n",
    "\n",
    "# Step 4: Print the new dataframe with all columns (but not all rows)\n",
    "print(\"Updated final_df shape:\", final_df.shape)\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values across all columns\n",
    "print(\"Missing values in dataset:\")\n",
    "print(final_df.isnull().sum())\n",
    "\n",
    "# Check for infinite values in numeric columns only\n",
    "numeric_df = final_df.select_dtypes(include=[np.number])\n",
    "print(\"\\nInfinite values in numeric columns:\")\n",
    "print(np.isinf(numeric_df).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN in final_df\n",
    "final_df = final_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# For numerical columns, fill missing values with the median.\n",
    "num_cols = final_df.select_dtypes(include=[np.number]).columns\n",
    "for col in num_cols:\n",
    "    final_df[col] = final_df[col].fillna(final_df[col].median())\n",
    "\n",
    "# For categorical columns, add \"missing\" to the categories (if not present) and fill missing values with \"missing\".\n",
    "cat_cols = final_df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    # Check if column's dtype is categorical using isinstance\n",
    "    if isinstance(final_df[col].dtype, pd.CategoricalDtype):\n",
    "        if \"missing\" not in final_df[col].cat.categories:\n",
    "            final_df[col] = final_df[col].cat.add_categories(\"missing\")\n",
    "    final_df[col] = final_df[col].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values across all columns\n",
    "print(\"Missing values in dataset:\")\n",
    "print(final_df.isnull().sum())\n",
    "\n",
    "# Check for infinite values in numeric columns only\n",
    "numeric_df = final_df.select_dtypes(include=[np.number])\n",
    "print(\"\\nInfinite values in numeric columns:\")\n",
    "print(np.isinf(numeric_df).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame as a Parquet file using pyarrow.\n",
    "final_df.to_parquet(\"../../datasets/final_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
