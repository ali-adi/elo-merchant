{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a226905b-1297-4a6b-902c-7b2172cb6c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for training, file operations, and plotting\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------- BASE DIRECTORY SETUP --------------------\n",
    "# Define the base folder for runs (should be at the same level as train.ipynb)\n",
    "BASE_RUNS_DIR = \"runs\"  # We'll create a new run folder when training starts\n",
    "\n",
    "# -------------------- HYPERPARAMETERS --------------------\n",
    "# Define training and model hyperparameters (adjust these easily for tuning)\n",
    "NUM_EPOCHS = 1        # For now, we run one epoch for testing the pipeline\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -------------------- DEVICE CONFIGURATION --------------------\n",
    "# Check if CUDA (GPU) is available; otherwise, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f5958f-15d3-4bb0-9179-2ba2273a1d9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df):\n",
    "    \"\"\"\n",
    "    Reduces the memory usage of a DataFrame by downcasting numeric types\n",
    "    and converting object columns to 'category' dtype if the number of unique values\n",
    "    is less than 50% of the total entries. This can help speed up computations and lower RAM usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Downcast integers\n",
    "        if col_type in ['int64', 'int32', 'int16']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        # Downcast floats\n",
    "        elif col_type in ['float64', 'float32', 'float16']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        # For object columns, convert to category if it saves space\n",
    "        elif col_type == 'object':\n",
    "            num_unique_values = df[col].nunique()\n",
    "            num_total_values = len(df[col])\n",
    "            # Convert to category if there are less than 50% unique values (adjust threshold if needed)\n",
    "            if num_unique_values / num_total_values < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8921a68-2be2-45bf-b428-13b401375844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15399755, 39)\n",
      "Valid shape: (3849939, 39)\n",
      "Test shape: (11825698, 38)\n",
      "Memory usage before optimization: 2135.22 MB\n",
      "Memory usage after optimization: 2135.22 MB\n",
      "Memory usage before optimization: 549.10 MB\n",
      "Memory usage after optimization: 549.10 MB\n",
      "Memory usage before optimization: 1488.98 MB\n",
      "Memory usage after optimization: 1488.98 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------- DATA LOADING --------------------\n",
    "# Define the path for datasets (train, valid, test are inside ../../datasets/split)\n",
    "DATA_DIR = \"../../datasets\"\n",
    "\n",
    "# Load train, validation, and test datasets using pyarrow engine for Parquet files\n",
    "train_df = pd.read_parquet(os.path.join(DATA_DIR, \"split/train.parquet\"), engine=\"pyarrow\")\n",
    "valid_df = pd.read_parquet(os.path.join(DATA_DIR, \"split/valid.parquet\"), engine=\"pyarrow\")\n",
    "test_df  = pd.read_parquet(os.path.join(DATA_DIR, \"test.parquet\"), engine=\"pyarrow\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Valid shape:\", valid_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# -------------------- MEMORY REDUCTION --------------------\n",
    "# Apply the memory reduction function to each dataset to optimize performance\n",
    "train_df = reduce_memory_usage(train_df)\n",
    "valid_df = reduce_memory_usage(valid_df)\n",
    "test_df = reduce_memory_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa4025b-f2c5-4d68-b457-655f3d1ebdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimensions (train, valid): (15399755, 37) (3849939, 37)\n"
     ]
    }
   ],
   "source": [
    "# -------------------- DATA PREPROCESSING --------------------\n",
    "# Define columns to drop from features (card_id is not used for training; we keep it for submission later if needed)\n",
    "drop_cols = ['card_id']\n",
    "target_col = 'target'\n",
    "\n",
    "# Prepare training features and labels\n",
    "X_train = train_df.drop(columns=drop_cols + [target_col], errors='ignore')\n",
    "y_train = train_df[target_col].values\n",
    "\n",
    "# Prepare validation features and labels\n",
    "X_valid = valid_df.drop(columns=drop_cols + [target_col], errors='ignore')\n",
    "y_valid = valid_df[target_col].values\n",
    "\n",
    "# Convert DataFrames to numpy arrays and ensure they are in float32 format (common for PyTorch)\n",
    "X_train = X_train.values.astype(np.float32)\n",
    "X_valid = X_valid.values.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32).reshape(-1, 1)\n",
    "y_valid = y_valid.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "print(\"Feature dimensions (train, valid):\", X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04bd4a3-c06c-4b1c-b2ae-cac5b6dc74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- CREATE DATASETS --------------------\n",
    "# Create TensorDatasets from the numpy arrays for both training and validation data\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "\n",
    "# -------------------- CREATE DATALOADERS --------------------\n",
    "# Create DataLoaders to iterate through the datasets in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990822e3-cdbf-479e-a4e8-068d914739b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "RegressionNN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -------------------- MODEL DEFINITION --------------------\n",
    "# Define a simple feed-forward neural network for regression using PyTorch\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        # Create a simple network with two hidden layers and ReLU activations\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output layer for regression (single value)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Determine the input dimension based on the training data features\n",
    "input_dim = X_train.shape[1]\n",
    "model = RegressionNN(input_dim).to(device)\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f612f7e-ccde-4630-bf2f-e010423a2f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory for this run: runs/20250318_145956\n",
      "\n",
      "Starting Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_X)        \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                 \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()                \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Accumulate weighted loss\u001b[39;00m\n",
      "File \u001b[0;32m~/root/University/Y4S2/SC4000/project/code/env/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/root/University/Y4S2/SC4000/project/code/env/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/root/University/Y4S2/SC4000/project/code/env/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # progress bar library\n",
    "\n",
    "# -------------------- CREATE OUTPUT DIRECTORY WHEN TRAINING STARTS --------------------\n",
    "current_run_dir = os.path.join(BASE_RUNS_DIR, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "os.makedirs(current_run_dir, exist_ok=True)\n",
    "OUTPUT_DIR = current_run_dir  # All outputs (model checkpoints, logs, plots, predictions) will be saved here\n",
    "print(\"Output directory for this run:\", OUTPUT_DIR)\n",
    "\n",
    "# -------------------- TRAINING SETUP --------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')\n",
    "checkpoint_path = os.path.join(OUTPUT_DIR, \"model_checkpoint.pth\")\n",
    "\n",
    "# -------------------- TRAINING LOOP --------------------\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nStarting Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over training batches with a progress bar\n",
    "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()         # Clear previous gradients\n",
    "        outputs = model(batch_X)        # Forward pass\n",
    "        loss = criterion(outputs, batch_y)  # Compute loss\n",
    "        loss.backward()                 # Backward pass\n",
    "        optimizer.step()                # Update weights\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)  # Accumulate weighted loss\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {epoch_train_loss:.4f}\")\n",
    "    \n",
    "    # -------------------- VALIDATION --------------------\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        # Use tqdm for validation progress as well\n",
    "        for batch_X, batch_y in tqdm(valid_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False):\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            valid_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {epoch_valid_loss:.4f}\")\n",
    "    \n",
    "    # -------------------- MODEL CHECKPOINTING --------------------\n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(\"Model checkpoint saved at\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a69b34-ca81-4e4d-bdf6-75ad4ea5a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- SAVE TRAINING LOGS --------------------\n",
    "# Create a DataFrame with the loss logs for each epoch\n",
    "log_df = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, NUM_EPOCHS+1)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"valid_loss\": valid_losses\n",
    "})\n",
    "\n",
    "# Define paths for saving logs (CSV and TXT)\n",
    "log_csv_path = os.path.join(OUTPUT_DIR, \"losses.csv\")\n",
    "log_txt_path = os.path.join(OUTPUT_DIR, \"losses.txt\")\n",
    "\n",
    "# Save the log DataFrame to a CSV file\n",
    "log_df.to_csv(log_csv_path, index=False)\n",
    "# Also save the log as a plain text file for easy viewing\n",
    "with open(log_txt_path, \"w\") as f:\n",
    "    f.write(log_df.to_string(index=False))\n",
    "\n",
    "print(\"Loss logs saved as CSV and TXT.\")\n",
    "\n",
    "# -------------------- GENERATE LOSS PLOTS --------------------\n",
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], marker='o', label='Train Loss')\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"valid_loss\"], marker='o', label='Valid Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as an image (PNG file)\n",
    "loss_plot_path = os.path.join(OUTPUT_DIR, \"loss_curve.png\")\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss plot saved as an image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d024237-8956-4403-aca1-21b1e670f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- TEST --------------------\n",
    "# Load the best model checkpoint (the one with the lowest validation loss)\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "# Preprocess test data: drop card_id if present (as it is an identifier)\n",
    "X_test = test_df.drop(columns=['card_id'], errors='ignore').values.astype(np.float32)\n",
    "X_test_tensor = torch.from_numpy(X_test).to(device)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Create a submission DataFrame including card_id if available\n",
    "if 'card_id' in test_df.columns:\n",
    "    submission = pd.DataFrame({\n",
    "        \"card_id\": test_df['card_id'],\n",
    "        \"prediction\": predictions.flatten()\n",
    "    })\n",
    "else:\n",
    "    submission = pd.DataFrame({\"prediction\": predictions.flatten()})\n",
    "\n",
    "# Define path to save test predictions\n",
    "submission_path = os.path.join(OUTPUT_DIR, \"test_predictions.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(\"Test predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b3945-e501-460e-8b2c-cf63fda8ed6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
